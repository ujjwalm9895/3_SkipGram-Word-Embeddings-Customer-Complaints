{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8ab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421c416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61acee0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9836bf",
   "metadata": {},
   "source": [
    "### Function to save and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449664ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name, obj):\n",
    "    \"\"\"\n",
    "    Function to save an object as pickle file\n",
    "    \"\"\"\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_file(name):\n",
    "    \"\"\"\n",
    "    Function to load a pickle object\n",
    "    \"\"\"\n",
    "    return pickle.load(open(name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430721a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac2d5",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effc0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "tokens_path = \"Output/tokens.pkl\"\n",
    "file_path = \"Input/complaints.csv\"\n",
    "col_name = \"Consumer complaint narrative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee7175fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Input/complaints.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\common.py:701\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    710\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Input/complaints.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd148f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6281c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25307207",
   "metadata": {},
   "source": [
    "### Drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in the specified column\n",
    "data.dropna(subset=[col_name], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd02f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the specified column from the DataFrame and assign it to the variable input_text\n",
    "input_text = data[col_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c1096",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a403a0a",
   "metadata": {},
   "source": [
    "### Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a713509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data in the list to lowercase while displaying a progress bar\n",
    "input_text = [i.lower() for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb923d",
   "metadata": {},
   "source": [
    "### Remove punctuations except apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation characters (except apostrophes) from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509c438",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcb906",
   "metadata": {},
   "source": [
    "### Remove 'xxxx' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove consecutive instances of 'x' from text in the list while displaying a progress bar\n",
    "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ba609",
   "metadata": {},
   "source": [
    "### Remove additional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace multiple consecutive spaces with a single space in text within the list while displaying a progress bar\n",
    "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3eb14",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24308af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text within the first 100 elements of input_text and store the tokens in the tokens list while displaying a progress bar\n",
    "tokens = [word_tokenize(t) for t in tqdm(input_text[:100])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39be9f",
   "metadata": {},
   "source": [
    "### Save tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(tokens_path, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51402e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a385d2",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "t = 1e-5\n",
    "context_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from Source.utils import save_file\n",
    "\n",
    "# Define the SkipGramDataset class\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_data, context_window=5, out_path=\"Output\", t=1e-5, k=10):\n",
    "        # Initialize the dataset\n",
    "        self.k = k\n",
    "        self.context_window = context_window\n",
    "\n",
    "        # Count the frequency of words in the input data\n",
    "        print(\"Counting word tokens...\")\n",
    "        counter = Counter([t for d in tqdm(input_data) for t in d])\n",
    "        self.vocab_count = len(counter)\n",
    "        print(f\"Unique words in the corpus: {self.vocab_count}\")\n",
    "\n",
    "        # Create positive data samples for Skip-gram\n",
    "        print(\"Creating data samples...\")\n",
    "        self.samples = self.positive_samples(input_data)\n",
    "\n",
    "        # Create vocabulary mapping and sampling probabilities\n",
    "        word2idx = dict()\n",
    "        idx2word = dict()\n",
    "        sampling_prob = []\n",
    "        print(\"Generating vocabulary...\")\n",
    "        for i, c in enumerate(counter.most_common(len(counter))):\n",
    "            word2idx[c[0]] = i\n",
    "            idx2word[i] = c[0]\n",
    "            sampling_prob.append(c[1])\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "\n",
    "        # Calculate and normalize sampling probabilities\n",
    "        print(\"Calculating sampling probabilities...\")\n",
    "        sampling_prob = np.sqrt(t/np.array(sampling_prob))\n",
    "        sampling_prob = sampling_prob / np.sum(sampling_prob)\n",
    "        self.sampling_prob = sampling_prob\n",
    "\n",
    "        # Save vocabulary mapping to files\n",
    "        print(\"Saving files...\")\n",
    "        self.save_files(out_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate negative samples and prepare the dataset for training\n",
    "        neg_words = self.negative_samples()\n",
    "        center_word = self.word2idx[self.samples.loc[idx, \"center_word\"]]\n",
    "        context_word = self.word2idx[self.samples.loc[idx, \"context_word\"]]\n",
    "        return torch.tensor(center_word), torch.tensor([context_word]+neg_words)\n",
    "\n",
    "    def positive_samples(self, input_data):\n",
    "        # Create positive data samples by defining context windows\n",
    "        samples = []\n",
    "        cw = self.context_window\n",
    "        for data in tqdm(input_data):\n",
    "            text = [None] * cw + data + [None] * cw\n",
    "            for i in range(cw, len(text) - cw):\n",
    "                samples.append((text[i], text[i - cw:i] + text[i + 1: i + cw + 1]))\n",
    "        samples = pd.DataFrame(samples, columns=[\"center_word\", \"context_word\"])\n",
    "        samples = samples.explode(\"context_word\")\n",
    "        samples.dropna(inplace=True)\n",
    "        samples.reset_index(drop=True, inplace=True)\n",
    "        return samples\n",
    "\n",
    "    def negative_samples(self):\n",
    "        # Generate negative samples for Skip-gram training\n",
    "        neg_words = list(np.random.choice(np.arange(self.vocab_count), self.k, p=self.sampling_prob))\n",
    "        return neg_words\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        # Save vocabulary mapping to files\n",
    "        save_file(os.path.join(out_path, \"word2idx.pkl\"), self.word2idx)\n",
    "        save_file(os.path.join(out_path, \"idx2word.pkl\"), self.idx2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57d120",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0d15b",
   "metadata": {},
   "source": [
    "# Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the SkipGram class\n",
    "class SkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, embedding_size=64):\n",
    "        # Initialize the SkipGram model\n",
    "        super(SkipGram, self).__init()\n",
    "\n",
    "        # Define the word embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_size)\n",
    "\n",
    "        # Initialize the weights matrix for Skip-gram\n",
    "        self.weights = torch.empty(embedding_size, vocab_len, requires_grad=True).type(torch.FloatTensor)\n",
    "        _ = torch.nn.init.normal_(self.weights)\n",
    "\n",
    "        # Define the output layer with LogSigmoid activation\n",
    "        self.out = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_word, context_words):\n",
    "        # Define the forward pass for Skip-gram\n",
    "        embeddings_ = self.embeddings(center_word)\n",
    "        weights_ = self.weights[:, context_words]\n",
    "        output = torch.einsum('bi,ibo->bo', embeddings_, weights_)\n",
    "        true_y = torch.zeros(output.shape[0], dtype=torch.int64)\n",
    "        return self.out(output), true_y\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        # Save the model's embeddings and weights to files\n",
    "        save_file(os.path.join(out_path, \"emb.pkl\"), self.embeddings)\n",
    "        save_file(os.path.join(out_path, \"weights.pkl\"), self.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f47d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative samples for Skip-gram training\n",
    "k = 10\n",
    "\n",
    "# Learning rate for model training\n",
    "lr = 0.01\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Batch size for training data\n",
    "batch_size = 128\n",
    "\n",
    "# Context window size for Skip-gram training\n",
    "context_window = 5\n",
    "\n",
    "# Output path for saving model and data files\n",
    "out_path = \"Output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a CUDA-compatible GPU is available; if not, use the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sg(dataloader, model, criterion, optimizer, device, num_epochs):\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize variables to track training progress\n",
    "    best_loss = 1e8  # A high initial value for tracking the best loss\n",
    "    patience = 0     # Counter for early stopping\n",
    "\n",
    "    # Loop over a specified number of training epochs\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = []  # List to store losses for each epoch\n",
    "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
    "        \n",
    "        # Iterate over the data loader (batches of training data)\n",
    "        for center_word, context_words in tqdm(dataloader):\n",
    "            center_word = center_word.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            \n",
    "            # Forward pass: compute model output and loss\n",
    "            output, true_y = model(center_word, context_words)\n",
    "            loss = criterion(output, true_y)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Append the loss for the current batch to the epoch_loss list\n",
    "            epoch_loss.append(loss.item())\n",
    "        \n",
    "        # Calculate the average loss for the current epoch\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        \n",
    "        # Update best_loss if the current epoch's loss is better\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        # Print the loss for the current epoch\n",
    "        print(f\"Loss: {epoch_loss}\")\n",
    "        \n",
    "        # Check for early stopping based on patience\n",
    "        if patience == 5:\n",
    "            print(\"Early stopping...\")\n",
    "    \n",
    "    # Save model files after training\n",
    "    model.save_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff902a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SkipGramDataset instance with the following parameters\n",
    "dataset = SkipGramDataset(input_data=tokens,  # Input data, typically tokenized text\n",
    "                          context_window=context_window,  # Size of the context window\n",
    "                          out_path=out_path,  # Output path for saving model files\n",
    "                          t=t,  # Threshold parameter for word sampling\n",
    "                          k=k)  # Number of negative samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch data loader for the SkipGramDataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset,  # The dataset to load\n",
    "                                         batch_size=batch_size,  # Batch size for training\n",
    "                                         shuffle=True,  # Shuffle the data in each epoch\n",
    "                                         drop_last=True)  # Drop the last batch if it's incomplete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SkipGram model with the specified vocabulary size and embedding size\n",
    "model = SkipGram(dataset.vocab_count, embedding_size=embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27590d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss criterion\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Initialize the optimizer with model parameters and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12580818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Skip-gram model\n",
    "train_sg(dataloader, model, criterion, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f91c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d5dd1",
   "metadata": {},
   "source": [
    "# Using embedings to get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word-to-index dictionary from a file\n",
    "word2idx = load_file(\"Output/word2idx.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[\"payments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945905c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_file(\"Output/emb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580df0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings(torch.tensor(83))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564248c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed477fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
